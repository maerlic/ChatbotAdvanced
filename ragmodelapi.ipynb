{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marco/miniconda3/envs/ragapplication_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "\u001b[33m * Tip: There are .env or .flaskenv files present. Do \"pip install python-dotenv\" to use them.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5002\n",
      " * Running on http://192.168.219.16:5002\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received query request\n",
      "Parsed request data\n",
      "Query text: Tell me about products that can help my skin?\n",
      "Querying the index\n",
      "Type of query results: <class 'llama_index.core.base.response.schema.StreamingResponse'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [06/Jun/2024 12:35:36] \"POST /query HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response data: ['\\n', '\\n', '', 'Based ', 'on ', 'the ', 'provided ', '', 'information, ', 'some ', 'products ', 'that ', 'could ', 'potentially ', 'help ', 'your ', 'skin ', '', 'include:\\n', '\\n', '', '', '1. ', '', '', '', \"Paula's \", 'Choice ', '', '', 'C15 ', 'Super ', '', 'Booster ', '', 'Vitamin ', 'C ', '', '', 'Serum: ', 'This ', '', 'vitamin ', 'C ', '', 'serum ', 'is ', 'known ', 'for ', 'its ', '', 'brightening ', 'properties ', 'and ', 'may ', 'help ', 'improve ', 'skin ', 'tone ', 'and ', 'reduce ', 'the ', 'appearance ', 'of ', 'fine ', 'lines ', 'and ', '', '', '', 'wrinkles.\\n', '\\n', '', '', '2. ', 'La ', '', '', '', 'Roche-Posay ', '', '', '', 'Anthelios ', '', '', '', 'Anti-Shine ', 'Dry ', 'Touch ', '', '', '', '', '', 'SPF50+: ', 'This ', '', 'sunscreen ', 'is ', 'designed ', 'to ', 'provide ', '', '', '', 'broad-spectrum ', 'protection ', 'against ', '', 'UVA ', 'and ', '', 'UVB ', '', 'rays ', 'while ', 'being ', '', 'lightweight ', 'and ', '', '', '', '', 'non-greasy. ', 'It ', 'may ', 'help ', 'protect ', 'your ', 'skin ', 'from ', 'damage ', 'caused ', 'by ', 'the ', '', 'sun.\\n', '\\n', '', '', '3. ', '', 'GAIA ', 'Natural ', 'Baby ', '', 'Skin ', '', '', 'Hydrating ', '', 'Lotion ', '', '', '', '', '', '150ml: ', 'This ', '', 'lotion ', 'is ', '', 'formulated ', 'specifically ', 'for ', '', \"babies' \", 'delicate ', 'skin ', 'and ', 'contains ', 'natural ', 'ingredients ', 'like ', '', 'shea ', 'butter ', 'and ', '', 'aloe ', '', 'vera ', 'to ', 'help ', '', 'soothe ', 'and ', '', '', 'nourish ', 'their ', '', 'skin.\\n', '\\n', '', '', '4. ', '', 'Aqua ', '', '', 'Hydrated ', '', '', 'Quenching ', '', 'Pore ', '', 'Balancing ', '', '', 'Cleansing ', '', '', 'Foam: ', 'This ', '', 'foaming ', '', 'cleanser ', 'is ', 'designed ', 'to ', 'remove ', '', '', 'impurities ', 'and ', 'excess ', 'oil ', 'from ', 'your ', 'skin ', 'without ', '', 'stripping ', 'away ', 'its ', 'natural ', '', 'moisture. ', 'It ', 'may ', 'help ', 'balance ', 'your ', '', '', \"skin's \", '', 'pH ', 'levels ', 'and ', 'prevent']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [06/Jun/2024 12:42:45] \"POST /reconstruct_index HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [06/Jun/2024 12:56:00] \"POST /reconstruct_index HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received query request\n",
      "Parsed request data\n",
      "Query text: What is the price of some of the items?\n",
      "Querying the index\n",
      "Type of query results: <class 'llama_index.core.base.response.schema.StreamingResponse'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [06/Jun/2024 12:57:27] \"POST /query HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response data: ['\\n', '', '', '1. ', '', '', '', 'DedCool ', '', 'Taunt ', 'Hand ', 'And ', 'Body ', '', '', 'Wash: ', '', '', '', '', '', '$23.00 ', '- ', '', '', '', '', '', '$46.00\\n', '', '', '2. ', '', '', '', 'DedCool ', '', 'Taunt ', 'Hand ', 'And ', 'Body ', '', 'Wash ', '', '', 'Refill: ', '', '', '', '', '', '$75.00\\n', '', '', '3. ', '', '', '', 'Nécessaire ', 'The ', 'Body ', '', 'Wash ', '', '', '', '', '', 'Eucalyptus: ', '', '', '', '', '', '$42.00 ', '- ', '', '', '', '', '', '$63.00\\n', '', '', '4. ', '', '', '', 'Nécessaire ', 'The ', 'Body ', '', 'Wash ', '', '', '', '', 'Eucalyptus ', '', '', 'Refill: ', '', '', '', '', '', '$58.00\\n', '\\n', '', 'These ', 'are ', 'the ', 'prices ', 'mentioned ', 'in ', 'the ', '', '', '', 'screenshot.']\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings, StorageContext, load_index_from_storage\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Check if GPU is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configure the global settings\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    model_name=\"h2oai/h2o-danube2-1.8b-chat\",\n",
    "    tokenizer_name=\"h2oai/h2o-danube2-1.8b-chat\"\n",
    ")\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"avsolatorio/NoInstruct-small-Embedding-v0\")\n",
    "Settings.chunk_size = 1024\n",
    "\n",
    "class LlamaIndexHelper:\n",
    "    def __init__(self, directory_path, embedding_model, delete_existing_index=False):\n",
    "        self.directory_path = directory_path\n",
    "        self.embedding_model = embedding_model\n",
    "        self.query_engine = self.construct_index(delete_existing_index)\n",
    "\n",
    "    def load_documents(self):\n",
    "        # Use the SimpleDirectoryReader to load documents from the specified directory\n",
    "        return SimpleDirectoryReader(self.directory_path).load_data()\n",
    "\n",
    "    def construct_index(self, delete_existing_index=False):\n",
    "        persist_dir = 'index_dir'\n",
    "\n",
    "        if delete_existing_index and os.path.exists(persist_dir):\n",
    "            shutil.rmtree(persist_dir)\n",
    "\n",
    "        os.makedirs(persist_dir, exist_ok=True)\n",
    "\n",
    "        if os.path.exists(persist_dir + '/docstore.json'):\n",
    "            storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "            if storage_context:\n",
    "                index = load_index_from_storage(storage_context)\n",
    "        else:\n",
    "            documents = self.load_documents()\n",
    "            index = VectorStoreIndex.from_documents(documents)\n",
    "            index.storage_context.persist(persist_dir)\n",
    "\n",
    "        return index.as_query_engine(streaming=True, similarity_top_k=3, verbose=True)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        text = text.replace(\"\\r\", \"\")\n",
    "\n",
    "        if len(text) == 0:\n",
    "            return None\n",
    "        return self.embedding_model.get_text_embedding(text)\n",
    "\n",
    "    def reconstruct_index(self):\n",
    "        self.query_engine = self.construct_index(delete_existing_index=True)\n",
    "\n",
    "@app.route('/query', methods=['POST'])\n",
    "def query():\n",
    "    try:\n",
    "        print(\"Received query request\")\n",
    "        data = request.json\n",
    "        print(\"Parsed request data\")\n",
    "        query_text = data.get('query')\n",
    "        print(f\"Query text: {query_text}\")\n",
    "        \n",
    "        if not query_text:\n",
    "            print(\"No query provided\")\n",
    "            return jsonify({'error': 'No query provided'}), 400\n",
    "        \n",
    "        # Use the query engine to get results\n",
    "        print(\"Querying the index\")\n",
    "        results = llama_helper.query_engine.query(query_text)\n",
    "        print(f\"Type of query results: {type(results)}\")\n",
    "\n",
    "        # Process the streaming response\n",
    "        response_data = []\n",
    "        for chunk in results.response_gen:\n",
    "            response_data.append(chunk)\n",
    "\n",
    "        print(f\"Response data: {response_data}\")\n",
    "\n",
    "        return jsonify({'results': response_data})\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/reconstruct_index', methods=['POST'])\n",
    "def reconstruct_index():\n",
    "    try:\n",
    "        llama_helper.reconstruct_index()\n",
    "        return jsonify({'message': 'Index successfully reconstructed.'}), 200\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "def unload_llm_model():\n",
    "    global llm_model\n",
    "    print(\"Unloading LLM model...\")\n",
    "    del llm_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"LLM model unloaded.\")\n",
    "\n",
    "llama_helper = LlamaIndexHelper(directory_path='Knowledge Base', embedding_model=Settings.embed_model)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbotllama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
